{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import csv\n",
    "import enum\n",
    "import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load moveNet model\n",
    "model = hub.load(\"https://tfhub.dev/google/movenet/multipose/lightning/1\")\n",
    "movenet = model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_person(persons,frame):\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    for i in range(len(persons)):\n",
    "\n",
    "        y, x, c = frame.shape\n",
    "        shaped = np.squeeze(np.multiply(persons[i], [y,x,1]))\n",
    "        person_score = np.average(list(shaped[:,2]))\n",
    "        scores[i]=person_score\n",
    "\n",
    "    return max(scores, key=scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BodyPart(enum.Enum):\n",
    "    \n",
    "  \"\"\"Enum representing human body keypoints detected by pose estimation models.\"\"\"\n",
    "  NOSE = 0\n",
    "  LEFT_EYE = 1\n",
    "  RIGHT_EYE = 2\n",
    "  LEFT_EAR = 3\n",
    "  RIGHT_EAR = 4\n",
    "  LEFT_SHOULDER = 5\n",
    "  RIGHT_SHOULDER = 6\n",
    "  LEFT_ELBOW = 7\n",
    "  RIGHT_ELBOW = 8\n",
    "  LEFT_WRIST = 9\n",
    "  RIGHT_WRIST = 10\n",
    "  LEFT_HIP = 11\n",
    "  RIGHT_HIP = 12\n",
    "  LEFT_KNEE = 13\n",
    "  RIGHT_KNEE = 14\n",
    "  LEFT_ANKLE = 15\n",
    "  RIGHT_ANKLE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "#     this class preprocess poses samples, it predicts keypoints on the images \n",
    "#     and save those keypoints in a csv file for the later use in the classification task \n",
    "\n",
    "        def __init__(self, images_in_folder,\n",
    "                    csvs_out_path):\n",
    "            self._images_in_folder = images_in_folder\n",
    "            self._csvs_out_path = csvs_out_path\n",
    "            self._csvs_out_folder_per_class = 'csv_per_pose'\n",
    "            self._message = []\n",
    "            \n",
    "            if(self._csvs_out_folder_per_class not in os.listdir()):\n",
    "                os.makedirs(self._csvs_out_folder_per_class)\n",
    "            \n",
    "#             get list of pose classes\n",
    "            self._pose_class_names = sorted(\n",
    "                [n for n in os.listdir(images_in_folder)]\n",
    "            )\n",
    "\n",
    "        def class_names(self):\n",
    "            return self.pose_class_names\n",
    "\n",
    "        def process(self):\n",
    "            \n",
    "#             Preprocess the images in the given folder\n",
    "            for pose_class_name in self._pose_class_names:\n",
    "#                 paths for pose class\n",
    "                images_in_folder = os.path.join(self._images_in_folder, pose_class_name)\n",
    "                csv_out_path = os.path.join(self._csvs_out_folder_per_class,\n",
    "                                               pose_class_name + '.csv'\n",
    "                                           )\n",
    "\n",
    "                #               Detect landmarks in each images and write it to the csv files\n",
    "                with open(csv_out_path, 'w') as csv_out_file:\n",
    "                    csv_out_writer = csv.writer(csv_out_file,\n",
    "                                                delimiter=',',\n",
    "                                                quoting=csv.QUOTE_MINIMAL\n",
    "                                               )\n",
    "\n",
    "                    #             get the list of images\n",
    "                    image_names = sorted(\n",
    "                        [n for n in os.listdir(images_in_folder)]\n",
    "                    )\n",
    "                    for image_name in tqdm.tqdm(image_names):\n",
    "                        image_path = os.path.join(images_in_folder, image_name)\n",
    "\n",
    "                        image = cv2.imread(image_path)\n",
    "                        y, x, c = image.shape\n",
    "                        img = image.copy()\n",
    "                        img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 192,192)\n",
    "                        input_img = tf.cast(img, dtype=tf.int32)\n",
    "\n",
    "                        #get skeleton from movenet\n",
    "                        results = movenet(input_img)\n",
    "                        persons = results['output_0'].numpy()[:,:,:51].reshape((6,17,3))\n",
    "\n",
    "                        i = get_person(persons,image)\n",
    "                        pose_landmarks = np.squeeze(np.multiply(persons[i], [y,x,1]))\n",
    "\n",
    "                        # Save landmarks if all landmarks above than the threshold\n",
    "                        min_landmark_score = min(list(pose_landmarks[:,2]))\n",
    "                        should_keep_image = min_landmark_score >= 0.1\n",
    "                        if not should_keep_image:\n",
    "                            continue\n",
    "\n",
    "                        # writing the landmark coordinates to its csv files\n",
    "                        coord = pose_landmarks.flatten().astype(np.str).tolist()\n",
    "                        csv_out_writer.writerow([image_name] + coord)\n",
    "\n",
    "            # combine all per-csv class CSVs into a sigle csv file\n",
    "            all_landmarks_df = self.all_landmarks_as_dataframe()\n",
    "            all_landmarks_df.to_csv(self._csvs_out_path, index=False)\n",
    "\n",
    "            \n",
    "        def all_landmarks_as_dataframe(self):\n",
    "            # Merging all csv for each class into a single csv file\n",
    "            total_df = None\n",
    "            for class_index, class_name in enumerate(self._pose_class_names):\n",
    "                csv_out_path = os.path.join(self._csvs_out_folder_per_class,\n",
    "                                               class_name + '.csv'\n",
    "                                           )\n",
    "                per_class_df = pd.read_csv(csv_out_path, header=None)\n",
    "                \n",
    "                # Add the labels\n",
    "                per_class_df['class_no'] = [class_index]*len(per_class_df)\n",
    "                per_class_df['class_name'] = [class_name]*len(per_class_df)\n",
    "                \n",
    "                # Append the folder name to the filename first column\n",
    "                per_class_df[per_class_df.columns[0]] = class_name + '/' +  per_class_df[per_class_df.columns[0]]\n",
    "                \n",
    "                if total_df is None:\n",
    "                    total_df = per_class_df\n",
    "                else:\n",
    "                    total_df = pd.concat([total_df, per_class_df], axis=0)\n",
    "            \n",
    "            list_name = [[bodypart.name + '_x', bodypart.name + '_y', \n",
    "                  bodypart.name + '_score'] for bodypart in BodyPart]\n",
    "            \n",
    "            header_name = []\n",
    "            for columns_name in list_name:\n",
    "                header_name += columns_name\n",
    "            header_name = ['filename'] + header_name\n",
    "            header_map = { total_df.columns[i]: header_name[i]\n",
    "                             for i in range(len(header_name))\n",
    "                         }\n",
    "            \n",
    "            total_df.rename(header_map, axis=1, inplace=True)\n",
    "            \n",
    "            return total_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess training data\n",
    "\n",
    "images_in_folder = os.path.join('data\\dataset\\strokes', 'test')\n",
    "csvs_out_path = '5_train_data.csv'\n",
    "train_preprocessor = Preprocessor(\n",
    "    images_in_folder,\n",
    "    csvs_out_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/398 [00:00<?, ?it/s]C:\\Users\\jouin\\AppData\\Local\\Temp\\ipykernel_21880\\690180614.py:67: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  coord = pose_landmarks.flatten().astype(np.str).tolist()\n",
      "100%|██████████| 398/398 [00:19<00:00, 20.21it/s]\n",
      "100%|██████████| 1664/1664 [01:08<00:00, 24.32it/s]\n",
      "100%|██████████| 592/592 [00:24<00:00, 23.84it/s]\n",
      "100%|██████████| 1560/1560 [01:02<00:00, 25.00it/s]\n",
      "100%|██████████| 2219/2219 [01:28<00:00, 25.17it/s]\n"
     ]
    }
   ],
   "source": [
    "train_preprocessor.process()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "865d8b2eb28e274047ba64063dfb6a2aabf0dfec4905d304d7a76618dae6fdd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
